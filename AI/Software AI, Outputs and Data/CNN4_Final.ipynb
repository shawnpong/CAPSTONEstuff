{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2528402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "CSV_PATH      = \"classified_data_CNN4_Final.csv\"\n",
    "LABEL_COL     = \"label\"\n",
    "\n",
    "WINDOW        = 128\n",
    "HOP           = 1\n",
    "NUM_SEGMENTS  = 8\n",
    "STATS_LIST    = (\"mean\",\"std\",\"p2p\",\"energy\")\n",
    "\n",
    "PCA_VAR_KEEP  = 0.80\n",
    "\n",
    "BATCH_SIZE    = 64\n",
    "EPOCHS        = 40\n",
    "DROPOUT_RATE  = 0.5\n",
    "LR            = 2e-3\n",
    "\n",
    "# Class-0 injection to enforce a strong \"none\" prior\n",
    "SYNTH_ZERO_TARGET_FRAC = 0.7\n",
    "SYNTH_ZERO_METHOD      = \"shuffle\"  # or \"gaussian\"\n",
    "GAUSS_ATTENUATION      = 0.5\n",
    "RNG_SEED               = 1337\n",
    "\n",
    "ARTIFACT_DIR  = \"artifacts_CNN4_Final\"\n",
    "Path(ARTIFACT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "HLS_PARAMS_DIR  = ARTIFACT_DIR + \"/HLS_PARAMS\"\n",
    "Path(HLS_PARAMS_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfe98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows: (47809, 128, 30) Labels: (47809,)\n"
     ]
    }
   ],
   "source": [
    "# Load CSV and create rolling windows\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "feature_cols = [c for c in df.columns if c not in (\"t\", LABEL_COL)]\n",
    "assert len(feature_cols) == 30, f\"Expected 30 raw channels, got {len(feature_cols)}\"\n",
    "X_raw = df[feature_cols].values.astype(np.float32)\n",
    "y_raw = df[LABEL_COL].astype(int).values\n",
    "\n",
    "def make_windows(X, y, win=WINDOW, hop=HOP):\n",
    "    Xw, yw = [], []\n",
    "    for end in range(win, len(X)+1, hop):\n",
    "        st = end - win\n",
    "        Xw.append(X[st:end])\n",
    "        yw.append(y[end-1])\n",
    "    return np.array(Xw, np.float32), np.array(yw, int)\n",
    "\n",
    "Xw, yw = make_windows(X_raw, y_raw)\n",
    "print(\"Windows:\", Xw.shape, \"Labels:\", yw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ead15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synth class-0 added: 6647\n",
      "class-0 before: 31472/54456 (65.8%) -> after: 38119/54456 (70.0%)\n"
     ]
    }
   ],
   "source": [
    "# Inject synthetic class-0 windows\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "def _count_frac_zero(y):\n",
    "    n0 = int((y == 0).sum())\n",
    "    return n0, n0/len(y)\n",
    "\n",
    "def _need_zeros(n_total, n0_cur, target_frac):\n",
    "    t = min(target_frac, 0.999)\n",
    "    return max(0, int(math.floor((t*n_total - n0_cur)/max(1e-9, 1.0 - t))))\n",
    "\n",
    "n0_before, frac0_before = _count_frac_zero(yw)\n",
    "n_add = _need_zeros(len(yw), n0_before, SYNTH_ZERO_TARGET_FRAC)\n",
    "\n",
    "def synth_gauss(n, W, C, atten=0.5):\n",
    "    if n <= 0: return np.empty((0,W,C), np.float32)\n",
    "    mu = X_raw.mean(axis=0).astype(np.float32)\n",
    "    sd = (X_raw.std(axis=0)+1e-6).astype(np.float32)\n",
    "    r = np.random.randn(n,W,C).astype(np.float32)*sd + mu\n",
    "    return mu + atten*(r - mu)\n",
    "\n",
    "def synth_shuffle(n, W, C):\n",
    "    if n <= 0: return np.empty((0,W,C), np.float32)\n",
    "    out = np.empty((n,W,C), np.float32)\n",
    "    starts = np.random.randint(0, len(X_raw)-W, size=n)\n",
    "    for i,s in enumerate(starts):\n",
    "        w = X_raw[s:s+W].copy()\n",
    "        perm = np.random.permutation(W)\n",
    "        out[i] = w[perm]\n",
    "    return out\n",
    "\n",
    "Xw_s0 = synth_gauss(n_add, WINDOW, X_raw.shape[1], GAUSS_ATTENUATION) if SYNTH_ZERO_METHOD==\"gaussian\" else synth_shuffle(n_add, WINDOW, X_raw.shape[1])\n",
    "yw_s0 = np.zeros((Xw_s0.shape[0],), int)\n",
    "\n",
    "if Xw_s0.shape[0] > 0:\n",
    "    Xw = np.concatenate([Xw, Xw_s0], axis=0).astype(np.float32)\n",
    "    yw = np.concatenate([yw, yw_s0], axis=0).astype(int)\n",
    "    perm = np.random.permutation(len(yw))\n",
    "    Xw, yw = Xw[perm], yw[perm]\n",
    "\n",
    "n0_after, frac0_after = _count_frac_zero(yw)\n",
    "print(f\"Synth class-0 added: {Xw_s0.shape[0]}\")\n",
    "print(f\"class-0 before: {n0_before}/{len(yw)} ({frac0_before:.1%}) -> after: {n0_after}/{len(yw)} ({frac0_after:.1%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f7d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized: (54456, 8, 120) Flat: (54456, 960)\n"
     ]
    }
   ],
   "source": [
    "# Temporally summarize (8 segments × stats)\n",
    "def summarize(win, num_segments=NUM_SEGMENTS, stats=STATS_LIST):\n",
    "    W, C = win.shape\n",
    "    seg_len = W // num_segments\n",
    "    feats = []\n",
    "    for s in range(num_segments):\n",
    "        a, b = s*seg_len, (s+1)*seg_len\n",
    "        seg = win[a:b]\n",
    "        parts = []\n",
    "        if \"mean\"   in stats: parts.append(seg.mean(axis=0))\n",
    "        if \"std\"    in stats: parts.append(seg.std(axis=0)+1e-8)\n",
    "        if \"p2p\"    in stats: parts.append(seg.max(axis=0)-seg.min(axis=0))\n",
    "        if \"energy\" in stats: parts.append((seg**2).sum(axis=0))\n",
    "        feats.append(np.concatenate(parts, axis=0))\n",
    "    return np.stack(feats, axis=0)\n",
    "\n",
    "sum_feats = np.array([summarize(w) for w in Xw])\n",
    "N, T, F = sum_feats.shape\n",
    "flat = sum_feats.reshape(N, -1).astype(np.float32)\n",
    "print(\"Summarized:\", sum_feats.shape, \"Flat:\", flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (38119, 34, 1) Val: (8168, 34, 1) Test: (8169, 34, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale -> PCA -> flatten\n",
    "scaler = StandardScaler().fit(flat)\n",
    "flat_scaled = scaler.transform(flat)\n",
    "\n",
    "pca = PCA(n_components=PCA_VAR_KEEP, svd_solver=\"full\")\n",
    "flat_pca = pca.fit_transform(flat_scaled)   # (N, D_pca)\n",
    "D_pca = flat_pca.shape[1]\n",
    "X_cnn = flat_pca.reshape(N, D_pca, 1).astype(np.float32)\n",
    "num_classes = int(yw.max()+1)\n",
    "\n",
    "\n",
    "# First: split 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_cnn, yw,\n",
    "    test_size=0.30,              # 30% goes to val+test\n",
    "    stratify=yw,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second: split temp into 15% val + 15% test\n",
    "# Since temp = 30%, the val/test split is 0.5\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,              # half of temp (15%)\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edad81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn1d_fp32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 34, 1)]           0         \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 34, 64)            256       \n",
      "                                                                 \n",
      " drop2 (Dropout)             (None, 34, 64)            0         \n",
      "                                                                 \n",
      " flat (Flatten)              (None, 2176)              0         \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 7)                 15239     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15495 (60.53 KB)\n",
      "Trainable params: 15495 (60.53 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define CNN model\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "def make_fp32_cnn(input_shape, n_classes, dropout=0.5):\n",
    "    x = layers.Input(shape=input_shape, name=\"input_fp32\")\n",
    "    x = layers.GaussianNoise(stddev=0.02, name=\"noise\")(x)\n",
    "    h = layers.Conv1D(64, 3, padding='same', activation='relu', name=\"conv1\")(x)\n",
    "    h = layers.Dropout(DROPOUT_RATE, name=\"drop2\")(h)\n",
    "    h = layers.Flatten(name=\"flat\")(h)\n",
    "    y = layers.Dense(n_classes, activation='softmax', name=\"softmax\")(h)\n",
    "    return models.Model(x, y, name=\"cnn1d_fp32\")\n",
    "\n",
    "\n",
    "fp32_model = make_fp32_cnn((D_pca,1), num_classes, dropout=DROPOUT_RATE)\n",
    "fp32_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(LR),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "fp32_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34dbce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.2270 - accuracy: 0.9231 - val_loss: 0.1240 - val_accuracy: 0.9569\n",
      "Epoch 2/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1531 - accuracy: 0.9457 - val_loss: 0.1166 - val_accuracy: 0.9569\n",
      "Epoch 3/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1406 - accuracy: 0.9499 - val_loss: 0.1075 - val_accuracy: 0.9606\n",
      "Epoch 4/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1345 - accuracy: 0.9522 - val_loss: 0.0977 - val_accuracy: 0.9640\n",
      "Epoch 5/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1325 - accuracy: 0.9518 - val_loss: 0.1014 - val_accuracy: 0.9631\n",
      "Epoch 6/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1297 - accuracy: 0.9539 - val_loss: 0.0936 - val_accuracy: 0.9649\n",
      "Epoch 7/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1242 - accuracy: 0.9549 - val_loss: 0.0950 - val_accuracy: 0.9654\n",
      "Epoch 8/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1194 - accuracy: 0.9563 - val_loss: 0.0950 - val_accuracy: 0.9669\n",
      "Epoch 9/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1245 - accuracy: 0.9548 - val_loss: 0.1005 - val_accuracy: 0.9634\n",
      "Epoch 10/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1155 - accuracy: 0.9587 - val_loss: 0.0867 - val_accuracy: 0.9673\n",
      "Epoch 11/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1197 - accuracy: 0.9570 - val_loss: 0.0901 - val_accuracy: 0.9668\n",
      "Epoch 12/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1179 - accuracy: 0.9567 - val_loss: 0.0891 - val_accuracy: 0.9674\n",
      "Epoch 13/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1162 - accuracy: 0.9584 - val_loss: 0.0870 - val_accuracy: 0.9654\n",
      "Epoch 14/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1129 - accuracy: 0.9595 - val_loss: 0.0906 - val_accuracy: 0.9671\n",
      "Epoch 15/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1140 - accuracy: 0.9582 - val_loss: 0.0893 - val_accuracy: 0.9669\n",
      "Epoch 16/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1139 - accuracy: 0.9584 - val_loss: 0.0937 - val_accuracy: 0.9635\n",
      "Epoch 17/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1113 - accuracy: 0.9592 - val_loss: 0.0915 - val_accuracy: 0.9658\n",
      "Epoch 18/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1126 - accuracy: 0.9594 - val_loss: 0.0813 - val_accuracy: 0.9701\n",
      "Epoch 19/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1117 - accuracy: 0.9597 - val_loss: 0.0829 - val_accuracy: 0.9689\n",
      "Epoch 20/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1103 - accuracy: 0.9598 - val_loss: 0.0836 - val_accuracy: 0.9680\n",
      "Epoch 21/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1130 - accuracy: 0.9589 - val_loss: 0.0832 - val_accuracy: 0.9677\n",
      "Epoch 22/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1098 - accuracy: 0.9597 - val_loss: 0.0825 - val_accuracy: 0.9687\n",
      "Epoch 23/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1092 - accuracy: 0.9601 - val_loss: 0.0790 - val_accuracy: 0.9716\n",
      "Epoch 24/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1077 - accuracy: 0.9612 - val_loss: 0.0850 - val_accuracy: 0.9682\n",
      "Epoch 25/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1082 - accuracy: 0.9604 - val_loss: 0.0833 - val_accuracy: 0.9689\n",
      "Epoch 26/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1073 - accuracy: 0.9602 - val_loss: 0.0818 - val_accuracy: 0.9709\n",
      "Epoch 27/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1072 - accuracy: 0.9606 - val_loss: 0.0855 - val_accuracy: 0.9688\n",
      "Epoch 28/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1090 - accuracy: 0.9606 - val_loss: 0.0850 - val_accuracy: 0.9689\n",
      "Epoch 29/40\n",
      "596/596 [==============================] - 3s 5ms/step - loss: 0.1062 - accuracy: 0.9616 - val_loss: 0.0863 - val_accuracy: 0.9695\n",
      "✅ FP32 Validation accuracy: 0.9716\n",
      "✅ Saved FP32 model: artifacts_CNN4_Final\\cnn1d_fp32_keras.keras\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True)\n",
    "hist = fp32_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "val_loss, val_acc = fp32_model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"✅ FP32 Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# (Optional) Save FP32 model\n",
    "fp32_path = os.path.join(ARTIFACT_DIR, \"cnn1d_fp32_keras.keras\")\n",
    "fp32_model.save(fp32_path)\n",
    "print(\"✅ Saved FP32 model:\", fp32_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdf16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved pca_params_summarizer.npz & meta.json\n"
     ]
    }
   ],
   "source": [
    "# Export PCA/scaler + meta\n",
    "np.savez(os.path.join(ARTIFACT_DIR, \"pca_params_summarizer.npz\"),\n",
    "         scaler_mean=np.asarray(scaler.mean_, dtype=np.float32),\n",
    "         scaler_scale=np.asarray(scaler.scale_, dtype=np.float32),\n",
    "         pca_components=np.asarray(pca.components_, dtype=np.float32),\n",
    "         pca_mean=np.asarray(pca.mean_, dtype=np.float32))\n",
    "\n",
    "meta = {\n",
    "    \"window\": int(WINDOW),\n",
    "    \"hop\": int(HOP),\n",
    "    \"orig_channels\": 30,\n",
    "    \"num_segments\": int(NUM_SEGMENTS),\n",
    "    \"stats_list\": list(STATS_LIST),\n",
    "    \"stats_per_segment\": int(len(STATS_LIST)),\n",
    "    \"F_flat\": int(scaler.mean_.size),\n",
    "    \"D_pca\": int(D_pca),\n",
    "    \"classes\": int(num_classes),\n",
    "    \"class_names\": [\"none\",\"come\",\"go\",\"turn\",\"pet\",\"feed\",\"fetch\"][:num_classes], #Mislabelled fetch. it's supposed to be throw ball\n",
    "    \"note\": \"FP32-trained model\"\n",
    "}\n",
    "with open(os.path.join(ARTIFACT_DIR, \"meta.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"✅ Saved pca_params_summarizer.npz & meta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ F1 Macro:    0.9523\n",
      "✅ F1 Weighted:0.9689\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        none       0.98      0.98      0.98      5718\n",
      "        come       0.92      0.96      0.94       315\n",
      "          go       0.97      0.96      0.96       444\n",
      "        turn       0.95      0.93      0.94       406\n",
      "         pet       0.97      0.94      0.95       313\n",
      "        feed       0.94      0.95      0.94       532\n",
      "       fetch       0.95      0.94      0.95       441\n",
      "\n",
      "    accuracy                           0.97      8169\n",
      "   macro avg       0.95      0.95      0.95      8169\n",
      "weighted avg       0.97      0.97      0.97      8169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reload model & compute F1 score\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Reload the saved FP32 model\n",
    "reload_model = tf.keras.models.load_model(fp32_path)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_probs = reload_model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Compute F1 score (macro and weighted)\n",
    "f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"✅ F1 Macro:    {f1_macro:.4f}\")\n",
    "print(f\"✅ F1 Weighted:{f1_weighted:.4f}\")\n",
    "\n",
    "# Full classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=meta[\"class_names\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04308cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected index: 2508\n",
      "Ground truth : 6 (fetch)\n",
      "Predicted     : 6 (fetch)\n",
      "Top-3 probs   : [6 0 3] [9.9974734e-01 2.5239616e-04 2.0592087e-07]\n",
      "✅ Wrote test header:  artifacts_CNN4_Final/HLS_PARAMS/tb_vector.h\n"
     ]
    }
   ],
   "source": [
    "# Non-zero test vector export for HLS\n",
    "\n",
    "# reload model\n",
    "reload_model = tf.keras.models.load_model(fp32_path)\n",
    "\n",
    "\n",
    "class_names = meta[\"class_names\"]\n",
    "CLASSES = meta[\"classes\"]\n",
    "D_IN = int(meta[\"D_pca\"])\n",
    "\n",
    "# pick a random NON-ZERO validation sample\n",
    "nz_idx = np.where(y_test != 0)[0]\n",
    "if nz_idx.size == 0:\n",
    "    # if all are zero, fallback to any sample\n",
    "    sel = np.random.randint(0, len(y_test))\n",
    "else:\n",
    "    sel = np.random.choice(nz_idx)\n",
    "\n",
    "x_sample = X_test[sel]\n",
    "y_true   = int(y_test[sel])\n",
    "\n",
    "# run inference\n",
    "probs = reload_model.predict(x_sample[None, ...], verbose=0)[0]\n",
    "y_pred = int(np.argmax(probs))\n",
    "\n",
    "print(f\"Selected index: {sel}\")\n",
    "print(f\"Ground truth : {y_true} ({class_names[y_true] if y_true < len(class_names) else y_true})\")\n",
    "print(f\"Predicted     : {y_pred} ({class_names[y_pred] if y_pred < len(class_names) else y_pred})\")\n",
    "print(\"Top-3 probs   :\", np.argsort(-probs)[:3], np.sort(probs)[-3:][::-1])\n",
    "\n",
    "test_in = x_sample.reshape(-1).astype(np.float32)\n",
    "assert test_in.shape[0] == D_IN\n",
    "\n",
    "tb_path = HLS_PARAMS_DIR + \"/tb_vector.h\"\n",
    "\n",
    "def _emit_1d_floats(name, arr):\n",
    "    lines = []\n",
    "    lines.append(f\"static const float {name}[{arr.shape[0]}] = \" + \"{\")\n",
    "    chunk = 8\n",
    "    for i in range(0, arr.shape[0], chunk):\n",
    "        seg = \", \".join(f\"{float(v):.8e}\" for v in arr[i:i+chunk])\n",
    "        lines.append(\"  \" + seg + (\",\" if i+chunk < arr.shape[0] else \"\"))\n",
    "    lines.append(\"};\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "with open(tb_path, \"w\") as f:\n",
    "    f.write(\"#pragma once\\n\")\n",
    "    f.write(\"// Auto-generated test vector for HLS testbench\\n\")\n",
    "    f.write(f\"#define TB_D_IN {D_IN}\\n\")\n",
    "    f.write(f\"#define TB_CLASSES {CLASSES}\\n\")\n",
    "    f.write(f\"#define TB_EXP_CLASS {y_pred}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(_emit_1d_floats(\"TB_TEST_IN\", test_in))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(_emit_1d_floats(\"TB_EXP_PROBS\", probs.astype(np.float32)))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Wrote test header: \", tb_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb3218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn1d_fp32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 34, 1)]           0         \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 34, 64)            256       \n",
      "                                                                 \n",
      " drop2 (Dropout)             (None, 34, 64)            0         \n",
      "                                                                 \n",
      " flat (Flatten)              (None, 2176)              0         \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 7)                 15239     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15495 (60.53 KB)\n",
      "Trainable params: 15495 (60.53 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "[Exporter] D_IN=34, KERNEL=3, CONV1_OUT=64, CLASSES=7\n",
      "✅ Export complete. Headers written to:  artifacts_CNN4_Final/HLS_PARAMS\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mdl = tf.keras.models.load_model(fp32_path)\n",
    "mdl.summary()\n",
    "\n",
    "layers = mdl.layers\n",
    "assert \"conv1\" in [l.name for l in layers], \"Expected a Conv1D layer named 'conv1'.\"\n",
    "conv = mdl.get_layer(\"conv1\")\n",
    "\n",
    "Wc, bc = conv.get_weights()\n",
    "KERNEL = Wc.shape[0]\n",
    "CIN    = Wc.shape[1]\n",
    "CONV1_OUT = Wc.shape[2]\n",
    "assert KERNEL == 3, f\"Expected kernel_size=3, got {KERNEL}\"\n",
    "assert CIN == 1,    f\"Expected input shape (*, D_pca, 1), got {CIN}\"\n",
    "\n",
    "dense = mdl.get_layer(\"softmax\")\n",
    "Wd, bd = dense.get_weights()\n",
    "FLAT = Wd.shape[0]\n",
    "CLASSES = Wd.shape[1]\n",
    "assert FLAT % CONV1_OUT == 0, \"Flattened size mismatch; check model architecture.\"\n",
    "D_IN = FLAT // CONV1_OUT\n",
    "\n",
    "print(f\"[Exporter] D_IN={D_IN}, KERNEL={KERNEL}, CONV1_OUT={CONV1_OUT}, CLASSES={CLASSES}\")\n",
    "\n",
    "CONV1_W = np.transpose(Wc[:, 0:1, :], (2, 0, 1)).reshape(CONV1_OUT, KERNEL).astype(np.float32)\n",
    "CONV1_B = bc.astype(np.float32)\n",
    "\n",
    "DENSE_W = Wd.astype(np.float32)\n",
    "DENSE_B = bd.astype(np.float32)\n",
    "\n",
    "def _emit_array_2d(f, name, arr):\n",
    "    rows, cols = arr.shape\n",
    "    f.write(f\"static const float {name}[{rows}][{cols}] = {{\\n\")\n",
    "    for r in range(rows):\n",
    "        vals = \", \".join(f\"{float(v):.8e}\" for v in arr[r])\n",
    "        f.write(f\"  {{ {vals} }},\\n\")\n",
    "    f.write(\"};\\n\")\n",
    "\n",
    "def _emit_array_1d(f, name, arr):\n",
    "    n = arr.shape[0]\n",
    "    vals = \", \".join(f\"{float(v):.8e}\" for v in arr)\n",
    "    f.write(f\"static const float {name}[{n}] = {{ {vals} }};\\n\")\n",
    "\n",
    "with open(HLS_PARAMS_DIR + \"/params.h\", \"w\") as f:\n",
    "    f.write(\"#pragma once\\n\")\n",
    "    f.write(f\"#define D_IN {D_IN}\\n\")\n",
    "    f.write(f\"#define KERNEL {KERNEL}\\n\")\n",
    "    f.write(f\"#define CONV1_OUT {CONV1_OUT}\\n\")\n",
    "    f.write(f\"#define CLASSES {CLASSES}\\n\")\n",
    "\n",
    "with open(HLS_PARAMS_DIR + \"/conv1_weights.h\", \"w\") as f:\n",
    "    f.write(\"#pragma once\\n\")\n",
    "    _emit_array_2d(f, \"CONV1_W\", CONV1_W)\n",
    "\n",
    "with open(HLS_PARAMS_DIR + \"/conv1_bias.h\", \"w\") as f:\n",
    "    f.write(\"#pragma once\\n\")\n",
    "    _emit_array_1d(f, \"CONV1_B\", CONV1_B)\n",
    "\n",
    "with open(HLS_PARAMS_DIR + \"/dense_weights.h\", \"w\") as f:\n",
    "    f.write(\"#pragma once\\n\")\n",
    "    _emit_array_2d(f, \"DENSE_W\", DENSE_W)\n",
    "\n",
    "with open(HLS_PARAMS_DIR + \"/dense_bias.h\", \"w\") as f:\n",
    "    f.write(\"#pragma once\\n\")\n",
    "    _emit_array_1d(f, \"DENSE_B\", DENSE_B)\n",
    "\n",
    "print(\"✅ Export complete. Headers written to: \", HLS_PARAMS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14548b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glove-cnn-qat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
